{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "$$\n",
    "\n",
    "<h1> Part 2: Supervised learning: regression </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning, we have a bunch of input, $x$, output, $y$, pairs as data.\n",
    "\n",
    "The great thing about supervised learning is that the inputs and outputs could be almost anything,\n",
    "```\n",
    "x :: Image,  y :: Int               # Object recognition\n",
    "x :: Image,  y :: Matrix{Int}       # Image segmentation\n",
    "x :: Audio,  y :: Str               # Speech recognition\n",
    "x :; Str,    y :: Audio             # Text-to-speech\n",
    "```\n",
    "The goal is to learn something about the mapping from $x$ to $y$.\n",
    "\n",
    "One approach is to learn a function that maps from $x$ to a guess about the corresponding $y$.  This guess is called $\\hat{y}$,\n",
    "\n",
    "\\begin{align}\n",
    "  f(x) \\rightarrow \\hat{y}\n",
    "\\end{align}\n",
    "\n",
    "To learn $f$, we try to make $y$ from the data as similar as possible to the estimates, $\\hat{y}$.\n",
    "\n",
    "However, to choose this function, we require a measure of \"similiarity\".  One common example is the squared error,\n",
    "\n",
    "\\begin{align}\n",
    "  \\text{SE} &= \\b{y - \\hat{y}}^2\n",
    "\\end{align}\n",
    "\n",
    "However, this approach makes learning difficult for discrete objects: what's the error between two outputs \"car\" and \"truck\".\n",
    "\n",
    "Instead, a generic approach is to write a function that takes an $x$ and returns a distribution over $y$,\n",
    "\n",
    "\\begin{align}\n",
    "  f(x) \\rightarrow \\P{y| x}\n",
    "\\end{align}\n",
    "\n",
    "It is easier to fit such a distribution, because we can always maximise the probability of the $y$ that we saw in the data.\n",
    "\n",
    "Note: classification and regression are special types of supervised learning.  In classification, the output is, a class-label,\n",
    "```\n",
    "Y = Int     #classification\n",
    "```\n",
    "in regression, the output is one (or many) real values,\n",
    "```\n",
    "Y = Float   #regression\n",
    "```\n",
    "If the output is more complicated (e.g. a string, image or audio), then its neither regression or classification, its just supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Multivariate linear regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most useful and instructive supervised learning method is multivariate linear regression.\n",
    "\n",
    "The input for the $\\lambda$th data point is, $\\x$, is a vector and we consider the multi-output case, where the corresponding output, $\\y_\\lambda$ is also a vector.\n",
    "We assume that $\\y_\\lambda$ is Gaussian, conditioned on $\\x$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda, \\w} &= \\N{y_\\lambda; \\x_\\lambda \\cdot \\w, \\sigma^2}.\n",
    "\\end{align}\n",
    "\n",
    "It turns out to be easier to write this as a multivariate Gaussian over all outputs, $\\y$, jointly,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\y| \\X, \\w} &= \\N{\\y; \\X \\w, \\sigma^2 \\I}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the log-probability out in full,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L\\b{\\w} &= \\log \\P{\\y| \\X, \\w} = \\sum_{\\lambda} \\sb{ -\\tfrac{1}{2} \\log 2 \\pi \\sigma^2 - \\tfrac{1}{2 \\sigma^2} \\b{Y_{\\lambda} - \\sum_i X_{\\lambda i} w_i}^2}.\n",
    "\\end{align}\n",
    "\n",
    "The maximum likelihood estimate, $\\wh$, is the maximum of the log-likelihood,\n",
    "\n",
    "\\begin{align}\n",
    "  \\wh &= \\argmax_\\w \\L\\b{\\w}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the maximum, we take the gradient,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\w}]{w_\\alpha} &= - \\frac{1}{2 \\sigma^2} \\dd{w_\\alpha} \\sum_\\lambda \\b{y_\\lambda - \\sum_i X_{\\lambda i} w_i}^2.\n",
    "\\end{align}\n",
    "\n",
    "This looks hard.  But because we've written it in index notation, we can do everything in terms of standard calculus.  We start by applying the chain rule,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\w}]{w_\\alpha} &= - \\frac{1}{\\sigma^2} \\sum_{\\lambda} \\b{y_\\lambda - \\sum_i X_{\\lambda i} w_i} \\dd{w_\\alpha} \\b{y_\\lambda - \\sum_i X_{\\lambda i} w_i}.\n",
    "\\end{align}\n",
    "\n",
    "interlude: the Kronecker delta (which looks alot like an identity matrix),\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[w_i]{w_\\alpha} = \\delta_{i \\alpha}\\\\\n",
    "  \\delta_{i\\alpha} = \\begin{cases}\n",
    "    1 & \\text{ if } i = \\alpha\\\\\n",
    "    0 & \\text{ if } i \\neq \\alpha\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "thus,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\w}]{w_\\alpha} &= \\frac{1}{\\sigma^2} \\sum_{\\lambda} \\b{y_\\lambda - \\sum_i X_{\\lambda i} w_i} \\sum_i \\delta_{i \\alpha} X_{\\lambda i} .\n",
    "\\end{align}\n",
    "\n",
    "the $\\delta_{i \\alpha}$ picks out the $i=\\alpha$ term in the sum,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\w}]{w_\\alpha} &= \\frac{1}{\\sigma^2} \\sum_{\\lambda} \\b{y_\\lambda - \\sum_i X_{\\lambda i} w_i} X_{\\lambda \\alpha} .\n",
    "\\end{align}\n",
    "\n",
    "Finally, put everything back in vector/matrix notation,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\w}]{\\w} &= \\tfrac{1}{\\sigma^2} \\X^T \\b{\\y - \\X \\w}\n",
    "\\end{align}\n",
    "\n",
    "We could just follow the gradient uphill (and that's what we do in deep learning), but this is super-slow.  Instead, there's an answer that is much faster to compute: at the top of the hill, the gradient is zero,\n",
    "\n",
    "\\begin{align}\n",
    "  \\0 &= \\at{\\dd[\\L\\b{\\w}]{\\w}}_{\\w=\\wh}\\\\ \n",
    "  \\0 &= \\tfrac{1}{\\sigma^2} \\X^T \\b{\\y - \\X \\wh} \\\\\n",
    "  \\0 &= \\X^T \\y - \\X^T \\X \\wh \\\\\n",
    "  \\X^T \\X \\wh &= \\X^T \\y\n",
    "\\end{align}\n",
    "\n",
    "Note that we can't just solve for $\\wh$ using the inverse of $\\X^T$ because it is almost never square, so the inverse does not exist!  As, $\\X^T \\X$ is square, we can take its inverse (note that $\\X^T$ isn't square, so we can't take its inverse,)\n",
    "\n",
    "\\begin{align}\n",
    "  \\wh  &= \\b{\\X^T \\X}^{-1} \\X^T \\y \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "I've done it this way, because the translation into Numpy/PyTorch is most direct, but if you look things up, you will often find the transposes in different places.\n",
    "\n",
    "Lets write some code!  First, we can directly translate the above expression to give a method for fitting $\\Wh$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_Wh(X, Y):\n",
    "    return t.inverse(X.T @ X) @ X.T @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate some 1D \"fake data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb169e199c4340acbc70622508a2c9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N     = 100 # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.1 # output noise\n",
    "X     = t.randn(N, D)\n",
    "Wtrue = t.ones(D, 1)\n",
    "Y     = X @ Wtrue + sigma*t.randn(N, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.scatter(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wtrue = tensor([[1.]])\n",
      "Wh    = tensor([[0.9943]])\n"
     ]
    }
   ],
   "source": [
    "Wh = fit_Wh(X, Y)\n",
    "print(f\"Wtrue = {Wtrue.T}\")\n",
    "print(f\"Wh    = {Wh.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9b4b44cd044b18a0cbd9c0bc0a48e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.scatter(X, Y, label=\"data\")\n",
    "ax.plot(X, X@Wh, 'r', label=\"fitted line\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also generate some 2D \"fake data\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2ba771dfc54ebc90ed42999a7f6999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N     = 100 # number of datapoints\n",
    "D     = 2   # dimension of datapoints\n",
    "sigma = 0.3 # output noise\n",
    "X     = t.randn(N, D)\n",
    "Wtrue = t.ones(D, 1)\n",
    "Y     = X @ Wtrue + sigma*t.randn(N, 1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(\"$x_{\\lambda, 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda, 1}$\")\n",
    "ax.set_zlabel(\"$y_{\\lambda, 0}$\")\n",
    "ax.scatter(xs=X[:, 0], ys=X[:, 1], zs=Y[:, 0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wtrue = tensor([[1., 1.]])\n",
      "Wh    = tensor([[1.0719, 0.9913]])\n"
     ]
    }
   ],
   "source": [
    "Wh = fit_Wh(X, Y)\n",
    "print(f\"Wtrue = {Wtrue.T}\")\n",
    "print(f\"Wh    = {Wh.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb14793b253640f8b9231fafc880fb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(\"$x_{\\lambda, 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda, 1}$\")\n",
    "ax.set_zlabel(\"$y_{\\lambda, 0}$\")\n",
    "ax.scatter(X[:, 0], X[:, 1], Y[:, 0])\n",
    "\n",
    "Xp = t.tensor([\n",
    "    [-4., -4.],\n",
    "    [-4.,  4.],\n",
    "    [ 4., -4.],\n",
    "    [ 4.,  4.]\n",
    "])\n",
    "\n",
    "ax.plot_trisurf(\n",
    "    np.array(Xp[:, 0]), \n",
    "    np.array(Xp[:, 1]), \n",
    "    np.array((Xp @ Wh)[:, 0]), \n",
    "    color='r', \n",
    "    alpha=0.3\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are issues with linear regression, as formulated thus far.\n",
    "\n",
    "In particular, consider 1D data, generated with a bias,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda, w, b} &= \\N{\\y_\\lambda; x_\\lambda w + b, \\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "If we fit our old model, without a bias, it doesn't work,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124de772ed964012b4fea5da020a2df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N     = 100 # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.1 # output noise\n",
    "X     = t.rand(N, D)\n",
    "Wtrue = 2*t.ones(D, 1)\n",
    "btrue = 3\n",
    "Y     = X @ Wtrue + btrue + sigma*t.randn(N, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 7)\n",
    "ax.scatter(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wtrue = tensor([[2.]])\n",
      "Wh    = tensor([[6.5637]])\n"
     ]
    }
   ],
   "source": [
    "Wh = fit_Wh(X, Y)\n",
    "print(f\"Wtrue = {Wtrue.T}\")\n",
    "print(f\"Wh    = {Wh.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2c876279b442a8a0b7638bbeb97b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 7)\n",
    "ax.scatter(X, Y, label=\"data\")\n",
    "ax.plot(X, X@Wh, 'r', label=\"fitted line\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do?  Well, we could go through the big derivation above again, incorporating the bias.\n",
    "\n",
    "But this is super-tedious.\n",
    "\n",
    "Instead, note that if we expand the 1D feature vector into a 2D feature vector, with the second feature being just biases,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda, w, b} &= \\N{\\y_\\lambda; \\underbrace{\\begin{pmatrix} x_\\lambda & 1 \\end{pmatrix}}_\\text{expanded feature vector} \\overbrace{\\begin{pmatrix} w \\\\ b \\end{pmatrix}}^\\text{expanded weight vector}, \\sigma^2}.\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda, w, b} &= \\N{\\y_\\lambda; x_\\lambda w + b, \\sigma^2}.\n",
    "\\end{align}\n",
    "\n",
    "Now, we can just use our original derivation, and implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1524, 1.0000],\n",
       "        [0.1655, 1.0000],\n",
       "        [0.7272, 1.0000],\n",
       "        [0.1338, 1.0000],\n",
       "        [0.5255, 1.0000],\n",
       "        [0.5009, 1.0000],\n",
       "        [0.5894, 1.0000],\n",
       "        [0.4173, 1.0000],\n",
       "        [0.5666, 1.0000],\n",
       "        [0.5193, 1.0000]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_bias(X):\n",
    "    return t.cat([X, t.ones(X.shape[0], 1)], 1)\n",
    "\n",
    "Xe = add_bias(X)\n",
    "Xe[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh    = tensor([[2.1170, 2.9391]])\n",
      "Wtrue = tensor([[2.]])\n",
      "btrue = 3\n"
     ]
    }
   ],
   "source": [
    "Wh = fit_Wh(Xe, Y)\n",
    "print(f\"Wh    = {Wh.T}\")\n",
    "print(f\"Wtrue = {Wtrue}\")\n",
    "print(f\"btrue = {btrue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74cec9e8e844ca4914c50ee18352253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 7)\n",
    "ax.scatter(X, Y, label=\"data\")\n",
    "\n",
    "xs = t.tensor([[0.], [1.]])\n",
    "ax.plot(xs, add_bias(xs)@Wh, 'r', label=\"fitted line\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that this idea can be taken *much* further.\n",
    "\n",
    "In particular, instead of just incorporating a constant feature, we can incorporate arbitrary, nonlinear *functions* of the original input data.\n",
    "\n",
    "To take an example, consider summing a quadratic, linear and constant function,\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda, w, b} &= \\N{\\y_\\lambda; \\underbrace{\\begin{pmatrix} x_\\lambda^2 & x_\\lambda & 1 \\end{pmatrix}}_\\text{expanded feature vector} \\overbrace{\\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{pmatrix}}^\\text{expanded weight vector}, \\sigma^2}.\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda, w, b} &= \\N{\\y_\\lambda; w_1 x_\\lambda^2 + w_2 x_\\lambda + w_3, \\sigma^2}.\n",
    "\\end{align}\n",
    "\n",
    "For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e31bf2d7b24994875719d6a6a5a512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N     = 100 # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.5 # output noise\n",
    "X     = t.randn(N, D)\n",
    "qtrue = 2  # quadratic term\n",
    "ltrue = -1 # linear term\n",
    "btrue = 1  # bias\n",
    "Y     = qtrue*X**2 + ltrue*X + btrue + sigma*t.randn(N, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.scatter(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6271e-01, -6.0225e-01,  1.0000e+00],\n",
       "        [ 6.4932e+00, -2.5482e+00,  1.0000e+00],\n",
       "        [ 5.2806e-01, -7.2668e-01,  1.0000e+00],\n",
       "        [ 4.5884e+00, -2.1421e+00,  1.0000e+00],\n",
       "        [ 1.0616e+00, -1.0303e+00,  1.0000e+00],\n",
       "        [ 1.3931e-02, -1.1803e-01,  1.0000e+00],\n",
       "        [ 4.2571e-03, -6.5246e-02,  1.0000e+00],\n",
       "        [ 4.1481e-01, -6.4406e-01,  1.0000e+00],\n",
       "        [ 6.9309e-01, -8.3252e-01,  1.0000e+00],\n",
       "        [ 8.1048e-01,  9.0027e-01,  1.0000e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quad(X):\n",
    "    return t.cat([X**2, X, t.ones(X.shape[0], 1)], 1)\n",
    "\n",
    "Xe = quad(X)\n",
    "Xe[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh    = tensor([[ 1.9982, -0.9738,  0.9850]])\n",
      "qtrue = 2\n",
      "ltrue = -1\n",
      "btrue = 1\n"
     ]
    }
   ],
   "source": [
    "Wh = fit_Wh(Xe, Y)\n",
    "print(f\"Wh    = {Wh.T}\")\n",
    "print(f\"qtrue = {qtrue}\")\n",
    "print(f\"ltrue = {ltrue}\")\n",
    "print(f\"btrue = {btrue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e206e4bccffb4857bd8df0705956c7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "\n",
    "ax.scatter(X, Y, label=\"data\")\n",
    "\n",
    "xs = t.linspace(-4, 4, 100)[:, None]\n",
    "ax.plot(xs, quad(xs)@Wh, 'r', label=\"fitted line\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
