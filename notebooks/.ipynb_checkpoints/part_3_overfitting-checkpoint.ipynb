{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib widget\n",
    "import ipywidgets\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "$$\n",
    "\n",
    "<h1> Part 3: Overfitting, regularisation and cross-validation </h1>\n",
    "\n",
    "<h2> Varieties of overfitting </h2>\n",
    "\n",
    "<h3> Too little data </h3>\n",
    "If we have too little data, then our inferences can be very wrong, even with a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d21ecc3ab5409b93437c02c6d67b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bias(X):\n",
    "    return t.cat([X, t.ones(X.shape[0], 1)], 1)\n",
    "def fit_Wh(X, Y):\n",
    "    return t.inverse(X.T @ X) @ X.T @ Y\n",
    "\n",
    "def plot():\n",
    "    N     = 3   # number of datapoints\n",
    "    D     = 1   # dimension of datapoints\n",
    "    sigma = 0.5 # output noise\n",
    "    X     = t.tensor([[-0.1], [0], [0.1]])\n",
    "    Xe    = bias(X)\n",
    "    Wtrue = t.tensor([[2.], [-1]])\n",
    "    Y     = Xe @ Wtrue + sigma*t.randn(N, 1)\n",
    "    Wh    = fit_Wh(Xe, Y)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_\\lambda$\")\n",
    "    ax.set_ylabel(\"$y_\\lambda$\")\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    ax.scatter(X, Y, label=\"data\");\n",
    "\n",
    "    xs = t.linspace(-4, 4, 100)[:, None]\n",
    "    ax.plot(xs, bias(xs)@Wtrue, 'b', label=\"true line\")\n",
    "    ax.plot(xs, bias(xs)@Wh, 'r', label=\"fitted\")\n",
    "    ax.legend()\n",
    "    \n",
    "interact_manual(plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Too little data in some directions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60916d513d14820abef16f8aa10608c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot():\n",
    "    N     = 100   # number of datapoints\n",
    "    D     = 2   # dimension of datapoints\n",
    "    sigma = 0.5 # output noise\n",
    "    rand = t.randn(N, 1) #1D random noise\n",
    "    X     = t.cat([rand, -rand], 1) + 1E-3*t.randn(N, 2) #input are the noise and its neg. this forms a straight line plus some noise\n",
    "    Wtrue = t.tensor([[1.], [-1.]])\n",
    "    Y     = X @ Wtrue + sigma*t.randn(N, 1)\n",
    "    Wh    = fit_Wh(X, Y)\n",
    "    #so here we only get good estimates for values that lie on the plane\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_xlabel(\"$x_{\\lambda, 0}$\")\n",
    "    ax.set_ylabel(\"$x_{\\lambda, 1}$\")\n",
    "    ax.set_zlabel(\"$y_{\\lambda, 0}$\")\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    ax.set_zlim(-15, 15)\n",
    "    ax.scatter(X[:, 0], X[:, 1], Y[:, 0])\n",
    "\n",
    "    Xp = t.tensor([\n",
    "        [-4., -4.],\n",
    "        [-4.,  4.],\n",
    "        [ 4., -4.],\n",
    "        [ 4.,  4.]\n",
    "    ])\n",
    "\n",
    "    ax.plot_trisurf(\n",
    "        np.array(Xp[:, 0]), \n",
    "        np.array(Xp[:, 1]), \n",
    "        np.array((Xp @ Wh)[:, 0]), \n",
    "        color='r', \n",
    "        alpha=0.3\n",
    "    )\n",
    "    \n",
    "interact_manual(plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The function class is too complex </h3>\n",
    "\n",
    "For the sake of argument, we consider \"Chebyshev polynomials\".  These are defined on $-1 \\leq x \\leq 1$ and range from $-1 \\leq y \\leq 1$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2544ed0e1b954fc7a9c41d11a7a4a995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cheb(xs, c):\n",
    "    # c is int\n",
    "    coefs = c*[0] + [1]\n",
    "    return np.polynomial.chebyshev.chebval(xs, coefs)\n",
    "\n",
    "xs = np.linspace(-1, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1.2, 1.2)\n",
    "\n",
    "i = 0\n",
    "def add_line():\n",
    "    global i\n",
    "    ax.plot(xs, cheb(xs, i))\n",
    "    i += 1\n",
    "interact_manual(add_line);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebX(X, order):\n",
    "    assert (-1 <= X).all() and (X <= 1).all()\n",
    "    \n",
    "    xs = []\n",
    "    for c in range(order):\n",
    "        xs.append(cheb(X, c))\n",
    "    return t.cat(xs, 1)\n",
    "   \n",
    "N     = 10  # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.5 # output noise\n",
    "t.manual_seed(0)\n",
    "rand  = t.rand(N, 1)\n",
    "X     = 2*rand - 1\n",
    "Wtrue = t.tensor([[0.2], [0.5]])\n",
    "Y     = chebX(X, 2) @ Wtrue + sigma*t.randn(N, 1)\n",
    "\n",
    "\n",
    "def plot(order):\n",
    "    Xe    = chebX(X, order)\n",
    "    Wh    = fit_Wh(Xe, Y)\n",
    "    print(f\"Wtrue = {Wtrue.T}\")\n",
    "    print(f\"Wh = {Wh.T}\")\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_\\lambda$\")\n",
    "    ax.set_ylabel(\"$y_\\lambda$\")\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X, Y, label=\"data\");\n",
    "\n",
    "    xs = t.linspace(-1, 1, 1000)[:, None]\n",
    "    ax.plot(xs, chebX(xs, order)@Wh, 'r', label=\"fitted\")\n",
    "    ax.legend()\n",
    "    \n",
    "interact_manual(plot, order=IntSlider(min=1, max=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Cross-validation </h2>\n",
    "\n",
    "How can we measure overfitting?\n",
    "\n",
    "The standard approach is cross-validation, where we split the data into \"training\" and \"validation\" sets.  We train the model on the training set, then look at the residuals/errors on the validation set.\n",
    "\n",
    "The model with the smallest cross-validation error wins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:7]\n",
    "X_test = X[7:]\n",
    "\n",
    "Y_train = Y[:7]\n",
    "Y_test = Y[7:]\n",
    "\n",
    "def plot(order):\n",
    "    Wh    = fit_Wh(chebX(X_train, order), Y_train)\n",
    "    \n",
    "    Yh_test = chebX(X_test, order) @ Wh\n",
    "    cross_validation_error = ((Y_test - Yh_test)**2).mean()\n",
    "    print(f\"cross validation error: {cross_validation_error}\")\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_\\lambda$\")\n",
    "    ax.set_ylabel(\"$y_\\lambda$\")\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X_train, Y_train, label=\"training data\");\n",
    "    ax.scatter(X_test, Y_test, label=\"validation data\");\n",
    "    ax.vlines(X_test, Y_test, Yh_test, label=\"test residuals\")\n",
    "    \n",
    "\n",
    "    xs = t.linspace(-1, 1, 1000)[:, None]\n",
    "    ax.plot(xs, chebX(xs, order)@Wh, 'r', label=\"fitted\")\n",
    "    ax.legend()\n",
    "    \n",
    "interact_manual(plot, order=IntSlider(min=1, max=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> k-fold cross-validation </h3>\n",
    "\n",
    "If you have relatively little data (as here), one thing you can do is to split your data up into train/validation sets in multiple different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Regularisation </h2>\n",
    "\n",
    "Cross-validation is great, but what if you've got lots of noise, and you can't control for that by just using a simple linear model?\n",
    "\n",
    "Then, you don't want to give up on using the more complex basis functions to capture non-linearities.\n",
    "\n",
    "Instead, we can penalise the weights, in particular,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(\\w) &= \\log \\P{\\y| \\X, \\w} - \\tfrac{1}{2} \\w^T \\La \\w\n",
    "\\end{align}\n",
    "\n",
    "where we could (if we wanted) penalise different weights differently, using the diagonal matrix, $\\La$.\n",
    "\n",
    "As an exercise (in problem sheet), the solution of the corresponding optimization problem is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Wh &= \\b{\\X^T \\X + \\sigma^2 \\La}^{-1} \\X \\y\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N     = 30  # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.2 # output noise\n",
    "t.manual_seed(0)\n",
    "rand  = t.rand(N, 1)\n",
    "X     = 2*rand - 1\n",
    "Wtrue = t.tensor([[0.], [0.], [0.], [0.], [0.3]])\n",
    "Y     = chebX(X, 5) @ Wtrue + sigma*t.randn(N, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-2, 2)\n",
    "\n",
    "ax.scatter(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:20]\n",
    "X_test = X[20:]\n",
    "\n",
    "Y_train = Y[:20]\n",
    "Y_test = Y[20:]\n",
    "\n",
    "def fit_reg_Wh(X, Y, reg):\n",
    "    # reg = sigma**2 * Lambda\n",
    "    return t.inverse(X.T @ X + reg*t.eye(X.shape[1])) @ X.T @ Y\n",
    "\n",
    "def plot(order, reg):\n",
    "    Wh    = fit_reg_Wh(chebX(X_train, order), Y_train, reg)\n",
    "\n",
    "    Yh_test = chebX(X_test, order) @ Wh\n",
    "    cross_validation_error = ((Y_test - Yh_test)**2).mean()\n",
    "    print(f\"cross validation error: {cross_validation_error}\")\n",
    "    print(f\"Wh: {Wh.T}\")\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_\\lambda$\")\n",
    "    ax.set_ylabel(\"$y_\\lambda$\")\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X_train, Y_train, label=\"training data\");\n",
    "    ax.scatter(X_test, Y_test, label=\"validation data\");\n",
    "    ax.vlines(X_test, Y_test, Yh_test, label=\"test residuals\")\n",
    "    \n",
    "\n",
    "    xs = t.linspace(-1, 1, 1000)[:, None]\n",
    "    ax.plot(xs, chebX(xs, order)@Wh, 'r', label=\"fitted\")\n",
    "    ax.legend()\n",
    "    \n",
    "interact_manual(plot, order=IntSlider(min=1, max=12), reg=FloatSlider(min=0, max=2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this example, the corollary of overfitting is the estimated weights getting really big.\n",
    "\n",
    "Regularisation explicitly penalises large weights, and gives more sensible solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Automatic selection of the regulariser using cross-validation </h3>\n",
    "\n",
    "Now things are even worse.  Before, we just had to choose the \"function complexity\", which was a smallish integer.  Now we have to choose a regulariser, and we don't even know what type of size the regulariser should be.\n",
    "\n",
    "The only thing we can do is to use cross-validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(order, reg):\n",
    "    Wh    = fit_reg_Wh(chebX(X_train, order), Y_train, reg)\n",
    "\n",
    "    Yh_test = chebX(X_test, order) @ Wh\n",
    "    return ((Y_test - Yh_test)**2).mean()\n",
    "\n",
    "log_10_regs = np.linspace(-4, 4, 100)\n",
    "regs = 10**log_10_regs\n",
    "cv_errors = np.array([cv(10, reg) for reg in regs])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"regulariser\")\n",
    "ax.set_ylabel(\"cross-validation error\")\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.plot(regs, cv_errors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can select the regulariser with the lowest cross-validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reg = regs[np.argmin(cv_errors)]\n",
    "plot(order=10, reg=best_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Limits of cross-validation </h3>\n",
    "\n",
    "There's a bunch of issues with cross-validation:\n",
    "<ul>\n",
    "    <li> Parameter sweeps can be numerically costly. </li>\n",
    "    <li> Splitting your data gives you less data for training, which is very problematic with smaller amounts of data. </li>\n",
    "    <li> Scales poorly if you want to cross-validate many different parameters. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
